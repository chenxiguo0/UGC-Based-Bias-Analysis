{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Successfully connected! Subreddit title: Python\n",
      "Read-only mode: True\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "client_id = 'XqvMFAQqrS1xWiUpCcbSOg'\n",
    "client_secret = '1WDnClQ3eo8Luzug2FSmZ6x9Aj1Ytg'\n",
    "user_agent = 'ios:MyRedditApp:v1.0 (by /u/sk_shuanq)'\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Test if connected\n",
    "print(reddit.read_only)  # Expected True\n",
    "\n",
    "try:\n",
    "    # Test with subreddit: python\n",
    "    subreddit = reddit.subreddit('python')\n",
    "    print(f\"Successfully connected! Subreddit title: {subreddit.title}\")\n",
    "    print(f\"Read-only mode: {reddit.read_only}\")  # True Expected\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text (Submission&Comments) Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for subreddit: Libertarian\n",
      "Skipping post with 'meme' flair: Statism in a Nutshell\n",
      "Skipping post with 'meme' flair: Welcome to the Twilight Zone\n",
      "Skipping post with 'meme' flair: How a Modern Day Mengele; became hugely wealthy promoting the NWO Agenda\n",
      "Fetching data for subreddit: LibertarianLeft\n",
      "Fetching data for subreddit: LibertarianSocialism\n",
      "Fetching data for subreddit: Conservative\n",
      "Fetching data for subreddit: conservatives\n",
      "Fetching data for subreddit: conservativeterrorism\n",
      "Fetching data for subreddit: politics\n",
      "Fetching data for subreddit: Republican\n",
      "Fetching data for subreddit: republicans\n",
      "Fetching data for subreddit: RepublicanValues\n",
      "Fetching data for subreddit: RepublicanPedophiles\n",
      "Fetching data for subreddit: democrats\n",
      "Skipping post with 'meme' flair: Forget \"of the people, by the people.\" America is now of the wealthy, by the corporations.\n",
      "Skipping post with 'meme' flair: No Man Should View Trump as a Role Model\n",
      "Fetching data for subreddit: DemocraticSocialism\n",
      "Fetching data for subreddit: worldnews\n",
      "Fetching data for subreddit: news\n",
      "Fetching data for subreddit: changemyview\n",
      "Fetching data for subreddit: Firearms\n",
      "Fetching data for subreddit: liberalgunowners\n",
      "Fetching data for subreddit: GunsAreCool\n",
      "Fetching data for subreddit: guncontrol\n",
      "Skipping post with 'meme' flair: ðŸ‡¦ðŸ‡º Never again, thankfully. ðŸ‡¦ðŸ‡º\n",
      "Skipping post with 'meme' flair: How can you distinguish someone who has a gun for self-defense from someone who has it gun in hopes they can use it? You canâ€™t.\n",
      "Skipping post with 'meme' flair: Photos from the March For Our Lives in Washington, DC on Saturday\n",
      "Fetching data for subreddit: lgbt\n",
      "Fetching data for subreddit: LGBTeens\n",
      "Fetching data for subreddit: LGBTnews\n",
      "Fetching data for subreddit: abortion\n",
      "Fetching data for subreddit: Abortiondebate\n",
      "Fetching data for subreddit: climatechange\n",
      "Fetching data for subreddit: ClimateOffensive\n",
      "Fetching data for subreddit: immigration\n",
      "Fetching data for subreddit: tax\n",
      "Fetching data for subreddit: FragileWhiteRedditor\n",
      "Fetching data for subreddit: racism\n",
      "Fetching data for subreddit: AskTrumpSupporters\n",
      "Fetching data for subreddit: AskConservatives\n",
      "Fetching data for subreddit: AskLibertarians\n",
      "Fetching data for subreddit: moderatepolitics\n",
      "Total posts fetched: 692\n",
      "Total comments fetched: 59032\n"
     ]
    }
   ],
   "source": [
    "file_path_subreddit = \"data/processed-data/subreddits.csv\"\n",
    "df_subreddit = pd.read_csv(file_path_subreddit)\n",
    "subreddit_list = df_subreddit['subreddit'].tolist()\n",
    "\n",
    "# Storage for results\n",
    "results = []\n",
    "\n",
    "# Counters for submission and comments\n",
    "submission_count = 0\n",
    "comment_count = 0\n",
    "\n",
    "# Function to convert UTC timestamp to readable format\n",
    "def convert_timestamp(utc_timestamp):\n",
    "    return datetime.fromtimestamp(utc_timestamp, timezone.utc).strftime('%Y/%m')\n",
    "\n",
    "# Iterate over each subreddit\n",
    "for subreddit_name in subreddit_list:\n",
    "    print(f\"Fetching data for subreddit: {subreddit_name}\")\n",
    "    try:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Store post IDs to avoid duplicates\n",
    "        seen_post_ids = set()\n",
    "        \n",
    "        # Submission Collection Part 1: Get top 10 posts by hot ranking\n",
    "        for submission in subreddit.hot(limit=10):\n",
    "            \n",
    "            # Skip repeat submission\n",
    "            if submission.id in seen_post_ids:\n",
    "                continue\n",
    "\n",
    "            # Skip posts with the \"meme\" flair\n",
    "            if submission.link_flair_text and \"meme\" in submission.link_flair_text.lower():\n",
    "                print(f\"Skipping post with 'meme' flair: {submission.title}\")\n",
    "                continue\n",
    "\n",
    "            submission_data = {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"post_id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"text\": submission.selftext,\n",
    "                \"score\": submission.score,\n",
    "                \"created_utc\": convert_timestamp(submission.created_utc),\n",
    "                \"sort_type\": \"hot\",\n",
    "                \"comments\": []\n",
    "            }\n",
    "\n",
    "            seen_post_ids.add(submission.id)\n",
    "            submission_count += 1\n",
    "\n",
    "            # Process comments for this submission\n",
    "            submission.comments.replace_more(limit=0)  \n",
    "            all_comments = submission.comments.list()\n",
    "            \n",
    "            # Sort comments by number of replies\n",
    "            comments_by_replies = sorted(\n",
    "                all_comments,\n",
    "                key=lambda x: len(x.replies) if hasattr(x, 'replies') else 0,\n",
    "                reverse=True\n",
    "            )[:10]\n",
    "            \n",
    "            # Process comments and add indentation\n",
    "            def process_comments(comments, depth=0):\n",
    "                global comment_count\n",
    "                processed_comments = []\n",
    "                \n",
    "                for comment in comments:\n",
    "                    # indent = \"  \" * depth  \n",
    "                    processed_comment = {\n",
    "                        \"comment_id\": comment.id,\n",
    "                        \"body\": comment.body,\n",
    "                        \"score\": comment.score,\n",
    "                        \"depth\": depth,\n",
    "                        \"num_replies\": len(comment.replies) if hasattr(comment, 'replies') else 0,\n",
    "                        \"created_utc\": convert_timestamp(comment.created_utc)  \n",
    "                    }\n",
    "                    \n",
    "                    comment_count += 1\n",
    "                    processed_comments.append(processed_comment)\n",
    "                    \n",
    "                    # Process replies if they exist\n",
    "                    if hasattr(comment, 'replies') and len(comment.replies) > 0:\n",
    "                        replies = list(comment.replies)[:10]  # Limit replies to 10 per comment\n",
    "                        processed_comments.extend(process_comments(replies, depth + 1))\n",
    "                \n",
    "                return processed_comments\n",
    "\n",
    "            # Process and store comments for the current submission\n",
    "            submission_data[\"comments\"] = process_comments(comments_by_replies)\n",
    "            results.append(submission_data)\n",
    "\n",
    "\n",
    "        # Submission Collection Part 2: Get top 10 posts by controversial ranking\n",
    "        for submission in subreddit.controversial(limit=10):\n",
    "            if submission.id in seen_post_ids:\n",
    "                continue\n",
    "                \n",
    "            # Skip posts with the \"meme\" flair\n",
    "            if submission.link_flair_text and \"meme\" in submission.link_flair_text.lower():\n",
    "                print(f\"Skipping post with 'meme' flair: {submission.title}\")\n",
    "                continue\n",
    "            \n",
    "            submission_data = {\n",
    "                \"subreddit\": subreddit_name,\n",
    "                \"post_id\": submission.id,\n",
    "                \"title\": submission.title,\n",
    "                \"text\": submission.selftext,\n",
    "                \"score\": submission.score,\n",
    "                \"created_utc\": convert_timestamp(submission.created_utc),  \n",
    "                \"sort_type\": \"controversial\",  \n",
    "                \"comments\": []\n",
    "            }\n",
    "            \n",
    "            seen_post_ids.add(submission.id)\n",
    "            submission_count += 1\n",
    "\n",
    "            # Process comments for this submission\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            all_comments = submission.comments.list()\n",
    "            \n",
    "            # Sort comments by number of replies\n",
    "            comments_by_replies = sorted(\n",
    "                all_comments,\n",
    "                key=lambda x: len(x.replies) if hasattr(x, 'replies') else 0,\n",
    "                reverse=True\n",
    "            )[:10]\n",
    "            \n",
    "            # Process and store comments using the same process_comments function\n",
    "            submission_data[\"comments\"] = process_comments(comments_by_replies)\n",
    "            results.append(submission_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print summary of fetched data\n",
    "print(f\"Total posts fetched: {submission_count}\")\n",
    "print(f\"Total comments fetched: {comment_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Results saved to data/raw-data/text_raw.json.\n"
     ]
    }
   ],
   "source": [
    "# Save results to a JSON file\n",
    "file_path_text_raw = \"data/raw-data/text_raw.json\"\n",
    "with open(file_path_text_raw, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Data collection complete. Results saved to {file_path_text_raw}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Format Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File convert complete. Results saved to data/processed-data/text_inital.json.\n",
      "Total rows: 59724\n",
      "\n",
      "Distribution by type:\n",
      "type\n",
      "comment                     59032\n",
      "submission_hot                347\n",
      "submission_controversial      345\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Depth distribution:\n",
      "depth\n",
      "0       692\n",
      "1      5573\n",
      "2     13747\n",
      "3     11078\n",
      "4      9039\n",
      "5      6583\n",
      "6      4939\n",
      "7      3411\n",
      "8      2353\n",
      "9      1495\n",
      "10      814\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>depth</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>1hf706u</td>\n",
       "      <td>submission_hot</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>The road to serfdom New to libertarian economi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m29svuv</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>Anything by Fredrich Bastiat is also good but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a5co6</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>Libertarians aren't exclusively anarchists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a694w</td>\n",
       "      <td>comment</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>True but like socialists and communists always...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a5co6</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>Libertarians aren't exclusively anarchists.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a694w</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>True but like socialists and communists always...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit       id            type  depth  score     time  \\\n",
       "0  Libertarian  1hf706u  submission_hot      0    100  2024/12   \n",
       "1  Libertarian  m29svuv         comment      1      1  2024/12   \n",
       "2  Libertarian  m2a5co6         comment      2      1  2024/12   \n",
       "3  Libertarian  m2a694w         comment      3      1  2024/12   \n",
       "4  Libertarian  m2a5co6         comment      1      1  2024/12   \n",
       "5  Libertarian  m2a694w         comment      2      1  2024/12   \n",
       "\n",
       "                                                text  \n",
       "0  The road to serfdom New to libertarian economi...  \n",
       "1  Anything by Fredrich Bastiat is also good but ...  \n",
       "2        Libertarians aren't exclusively anarchists.  \n",
       "3  True but like socialists and communists always...  \n",
       "4        Libertarians aren't exclusively anarchists.  \n",
       "5  True but like socialists and communists always...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the json data\n",
    "def convert_reddit_data(json_input_path, csv_output_path):\n",
    "    with open(json_input_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Lists to store the processed data\n",
    "    processed_data = []\n",
    "\n",
    "    # Process each submission and its comments\n",
    "    for submission in data:\n",
    "        submission_text = submission['title']\n",
    "        if submission['text']:  # Combine text content if it exists\n",
    "            submission_text += \" \" + submission['text']\n",
    "            \n",
    "        # Add submission data\n",
    "        processed_data.append({\n",
    "            'subreddit': submission['subreddit'],\n",
    "            'id': submission['post_id'],\n",
    "            'type': f\"submission_{submission['sort_type']}\",  # submission_hot or submission_con\n",
    "            'depth': 0,  # submissions are depth 0\n",
    "            'score': submission['score'],\n",
    "            'time': submission['created_utc'],\n",
    "            'text': submission_text\n",
    "        })\n",
    "        \n",
    "        # Process comments\n",
    "        for comment in submission['comments']:\n",
    "            processed_data.append({\n",
    "                'subreddit': submission['subreddit'],\n",
    "                'id': comment['comment_id'],\n",
    "                'type': 'comment',\n",
    "                'depth': comment['depth'] + 1,  # increment depth by 1\n",
    "                'score': comment['score'],\n",
    "                'time': comment['created_utc'],\n",
    "                'text': comment['body']\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    # Print some basic statistics\n",
    "    print(\"\\nFile convert complete. Results saved to data/processed-data/text_inital.json.\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(\"\\nDistribution by type:\")\n",
    "    print(df['type'].value_counts())\n",
    "    print(\"\\nDepth distribution:\")\n",
    "    print(df['depth'].value_counts().sort_index())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Execute the conversion\n",
    "file_path_text_raw = \"data/raw-data/text_raw.json\"\n",
    "file_path_text_initial = \"data/processed-data/text_initial.csv\"  \n",
    "\n",
    "df_text_initial = convert_reddit_data(file_path_text_raw, file_path_text_initial)\n",
    "df_text_initial.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/guochenxi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/guochenxi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning complete. Results saved to data/processed-data/text_clean.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>type</th>\n",
       "      <th>depth</th>\n",
       "      <th>score</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>1hf706u</td>\n",
       "      <td>submission_hot</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>road serfdom new libertarian economics dad let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m29svuv</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>anything fredrich bastiat also good think stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a5co6</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>libertarian arent exclusively anarchist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a694w</td>\n",
       "      <td>comment</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>true like socialist communist always saying fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m29bopi</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>yeah good favorite always recommend get starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Libertarian</td>\n",
       "      <td>m2a1cm0</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2024/12</td>\n",
       "      <td>salma write book becoming movie star</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit       id            type  depth  score     time  \\\n",
       "0  Libertarian  1hf706u  submission_hot      0    100  2024/12   \n",
       "1  Libertarian  m29svuv         comment      1      1  2024/12   \n",
       "2  Libertarian  m2a5co6         comment      2      1  2024/12   \n",
       "3  Libertarian  m2a694w         comment      3      1  2024/12   \n",
       "6  Libertarian  m29bopi         comment      1      7  2024/12   \n",
       "7  Libertarian  m2a1cm0         comment      1      1  2024/12   \n",
       "\n",
       "                                                text  \n",
       "0  road serfdom new libertarian economics dad let...  \n",
       "1  anything fredrich bastiat also good think stat...  \n",
       "2            libertarian arent exclusively anarchist  \n",
       "3  true like socialist communist always saying fi...  \n",
       "6  yeah good favorite always recommend get starte...  \n",
       "7               salma write book becoming movie star  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# Download NLTK stopwords and WordNet data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove newline characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace characters (including newline, tabs) with a single space\n",
    "    \n",
    "    # Remove meaningless characters, such as special symbols \n",
    "    text = re.sub(r'[^\\w\\s,.\\-]', '', text)  # Keep letters, numbers, spaces, commas, periods, and hyphens\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'@\\S+', '', text)  # Remove @username mentions\n",
    "    text = re.sub(r'#[\\w]+', '', text)  # Remove all hashtags (e.g., #aiart)\n",
    "    text = re.sub(r'-', '', text)  # Remove hyphens\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')  \n",
    "    \n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove double quotes\n",
    "    text = text.replace('\"', '')  \n",
    "\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "    # English words only\n",
    "    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Remove stop words and apply lemmatization\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    # Join the processed words back into a string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Clean the text of each row in the 'text' column\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: clean_text(x))  # Clean the 'text' field\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: remove_non_english_words(x))  # Keep only English words\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: process_text(x))  # Remove stop words and apply lemmatization\n",
    "\n",
    "# Drop duplicate rows based on the 'text' column\n",
    "df_text_clean = df_text_initial.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Output the cleaned DataFrame to a CSV file\n",
    "file_path_text_clean = \"data/processed-data/text_clean.csv\"\n",
    "df_text_clean.to_csv(file_path_text_clean, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Data cleaning complete. Results saved to {file_path_text_clean}\")\n",
    "df_text_clean.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5400",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
