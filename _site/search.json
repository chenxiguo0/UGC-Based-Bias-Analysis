[
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "Use this page to track your progress and keep a log of your contributions to the project, please update this each time you work on your project, it is generally a good habit to adopt.\nIf you are working as a team, at the end, you can duplicate the project and add it to your individual portfolio websites. If you do, you MUST retain attribution to your teammates. Removing attribution would constitute plagiarism."
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nwrite a technical methods sections for K-means\nwrite a technical methods sections for PCA\n\n… etc"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nT: 10-15-2024\n\nCoordinate with team member to set up weekly meeting time\n\nM: 10-14-2024\n\nDo a first draft of the project landing page"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024\n\nAttend first group meeting"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Landing page",
    "section": "",
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\n\n\n\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\n\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page."
  },
  {
    "objectID": "index.html#project-name",
    "href": "index.html#project-name",
    "title": "Landing page",
    "section": "",
    "text": "This is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\n\n\n\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\n\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Landing page",
    "section": "About Me",
    "text": "About Me\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects."
  },
  {
    "objectID": "index.html#literature-review",
    "href": "index.html#literature-review",
    "title": "Landing page",
    "section": "Literature Review",
    "text": "Literature Review\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Landing page",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "codes_subreddit_selection.html",
    "href": "codes_subreddit_selection.html",
    "title": "1. Subreddit Selection",
    "section": "",
    "text": "import praw\nimport pandas as pd\n\n\nInitialize Reddit API\n\nclient_id = 'XqvMFAQqrS1xWiUpCcbSOg'\nclient_secret = '1WDnClQ3eo8Luzug2FSmZ6x9Aj1Ytg'\nuser_agent = 'ios:MyRedditApp:v1.0 (by /u/sk_shuanq)'\n\nreddit = praw.Reddit(\n    client_id=client_id,\n    client_secret=client_secret,\n    user_agent=user_agent\n)\n\n# Test if connected\nprint(reddit.read_only)  # Expected True\n\ntry:\n    # Test with subreddit: python\n    subreddit = reddit.subreddit('python')\n    print(f\"Successfully connected! Subreddit title: {subreddit.title}\")\n    print(f\"Read-only mode: {reddit.read_only}\")  # True Expected\nexcept Exception as e:\n    print(f\"Connection failed: {e}\")\n\nTrue\nSuccessfully connected! Subreddit title: Python\nRead-only mode: True\n\n\n\nkeywords = [\n    'libertarian', 'conservative', 'politics', 'republican', 'democrat', \n    'news', 'view', 'gun control', 'lgbt', 'abortion', 'vaccine', \n    'climate change', 'immigration', 'tax', 'racism'\n]\n\n\n\nSearch for Keyword\n\n# List to store results\nsubreddit_list = []\n\n# Search for subreddits containing each keyword\nfor keyword in keywords:\n    print(f\"Searching for keyword: {keyword}\")\n    try:\n        # Fetch results with the keyword\n        search_results = reddit.subreddits.search_by_name(keyword, include_nsfw=False)  # filter Not Safe For Work content\n        \n        # Collect subreddit details\n        temp_results = []\n        for subreddit in search_results:\n            temp_results.append({\n                \"keyword\": keyword,\n                \"subreddit\": subreddit.display_name,\n                \"subscribers\": subreddit.subscribers,\n                \"description\": subreddit.public_description\n            })\n        \n        # Sort by subscribers and take the top 4\n        temp_results = sorted(temp_results, key=lambda x: x[\"subscribers\"], reverse=True)[:4]\n        \n        # Append to main list\n        subreddit_list.extend(temp_results)\n    except Exception as e:\n        print(f\"Error searching for keyword '{keyword}': {e}\")\n\nSearching for keyword: libertarian\nSearching for keyword: conservative\nSearching for keyword: politics\nSearching for keyword: republican\nSearching for keyword: democrat\nSearching for keyword: news\nSearching for keyword: view\nSearching for keyword: gun control\nSearching for keyword: lgbt\nSearching for keyword: abortion\nSearching for keyword: vaccine\nSearching for keyword: climate change\nSearching for keyword: immigration\nSearching for keyword: tax\nSearching for keyword: racism\n\n\n\n\nConvert and Save\n\n# Convert results to DataFrame\ndf = pd.DataFrame(subreddit_list)\n\n# Display the DataFrame\nprint(\"\\nTop Subreddits by Keyword:\")\nprint(df)\n\n# Save to CSV for future reference\nfile_path_subreddit_raw = \"data/raw-data/subreddits_raw.csv\"\ndf.to_csv(file_path_subreddit_raw, index=False)\n\n\nTop Subreddits by Keyword:\n           keyword              subreddit  subscribers  \\\n0      libertarian            Libertarian       503241   \n1      libertarian        libertarianmeme       147424   \n2      libertarian        LibertarianLeft        14930   \n3      libertarian   LibertarianSocialism        13957   \n4     conservative           Conservative      1154369   \n5     conservative          conservatives       108335   \n6     conservative  conservativeterrorism        85102   \n7     conservative      ConservativeMemes        75440   \n8         politics               politics      8687246   \n9         politics             ukpolitics       507858   \n10        politics     AustralianPolitics       236946   \n11        politics         CanadaPolitics       231836   \n12      republican             Republican       206488   \n13      republican            republicans        19836   \n14      republican       RepublicanValues        19490   \n15      republican   RepublicanPedophiles        10571   \n16        democrat              democrats       481711   \n17        democrat    DemocraticSocialism       155847   \n18        democrat        democraticparty         7337   \n19        democrat               Democrat         3685   \n20            news              worldnews     43237643   \n21            news                   news     29097502   \n22            news        NewsOfTheStupid       329463   \n23            news             NewSkaters       228673   \n24            view           changemyview      3750179   \n25            view            TradingView        94981   \n26            view          remoteviewing        78489   \n27            view              CrossView        66518   \n28     gun control               Firearms       273651   \n29     gun control       liberalgunowners       221503   \n30     gun control            GunsAreCool        56869   \n31     gun control             guncontrol        11959   \n32            lgbt                   lgbt      1139491   \n33            lgbt               LGBTeens       171901   \n34            lgbt              lgbtmemes       123743   \n35            lgbt               LGBTnews        83883   \n36        abortion               abortion        61246   \n37        abortion         Abortiondebate        10171   \n38        abortion    AbortionPillsByPost         1982   \n39        abortion   Abortion_Philippines          278   \n40         vaccine  vaccineautismevidence         8574   \n41         vaccine               VACCINES         7673   \n42         vaccine      vaccinelonghauler         6071   \n43         vaccine           CovidVaccine         5777   \n44  climate change          climatechange       119670   \n45  climate change       ClimateOffensive        73579   \n46  climate change        climate_science        20330   \n47  climate change  ClimateChangeSurprise          449   \n48     immigration      ImmigrationCanada       227904   \n49     immigration            immigration       144238   \n50     immigration   ImmigrationAustralia         7142   \n51     immigration         immigrationlaw         3137   \n52             tax                    tax       284378   \n53             tax                  taxPH        94819   \n54             tax               IndiaTax        92572   \n55             tax                taxpros        76239   \n56          racism       AccidentalRacism       450194   \n57          racism   FragileWhiteRedditor       239705   \n58          racism                 racism        47690   \n59          racism       racismdiscussion          513   \n\n                                          description  \n0   Welcome to /r/Libertarian, a subreddit to disc...  \n1   For an end to democracy and tyranny. For more ...  \n2   Liberty is the mother, not the daughter, of or...  \n3   A community for the discussion of news and lin...  \n4   The largest conservative subreddit.\\nhttps://d...  \n5   Conservatism (from conservare, \"to preserve\") ...  \n6   Documenting conservative terrorist actions and...  \n7   Become a 🔥🔥🔥 ConservativeMemes 🔥🔥🔥 subscriber!...  \n8   /r/Politics is for news and discussion about U...  \n9   Political news and debate concerning the Unite...  \n10  The purpose of this subreddit is civil and ope...  \n11        Polite discussions about Canadian politics.  \n12  /r/Republican is a partisan subreddit. This is...  \n13               PRO-REPUBLICAN SUBREDDIT FOR ADULTS!  \n14  A look at the values expressed by the modern R...  \n15  This community documents cases where republica...  \n16  The Democratic Party is building a better futu...  \n17  Whether you're a Progressive, Marxist, or a De...  \n18  Your premier stop for all things related to th...  \n19  The Democratic Party is currently the party of...  \n20  A place for major news from around the world, ...  \n21  The place for news articles about current even...  \n22  Did you hear about the man who butt-dialed 911...  \n23  Welcome to New Skaters! A place where ALL new ...  \n24  A place to post an opinion you accept may be f...  \n25  This is the \"WTF, TradingView?!\" community for...  \n26  Remote viewing (RV) is the practice of seeking...  \n27  Cross viewing is seeing 3D with nothing but yo...  \n28  Discuss firearms, politics, 2nd amendment news...  \n29  Gun-ownership through a pro-gun liberal / left...  \n30  The cost of 'cool'. Mass Shooter Tracker Data....  \n31  We're a well-regulated sub that accepts the sc...  \n32  A safe space for GSRM (Gender, Sexual, and Rom...  \n33  A place where LGBTeens and LGBT allies can han...  \n34             Your sub for everything LGBTQ+ memes!   \n35  r/LGBTnews is for sharing links to recent news...  \n36  If you're pregnant and don't want to be, we ca...  \n37  Welcome to the Abortion Debate subreddit!\\n\\nT...  \n38  Mifepristone + Misoprostol by post 24/7 in the...  \n39                                                     \n40  This subreddit is dedicated to archiving all o...  \n41  This subreddit is committed to answering quest...  \n42  This reddit is for people who have received on...  \n43                                                     \n44  This is a place for the rational discussion of...  \n45  We're here to do something about climate chang...  \n46                                    Climate science  \n47  Airports closed by heat, Arctic seed vaults fl...  \n48  The Canadian Immigration Subreddit.  This subr...  \n49  A place to discuss US and Worldwide immigratio...  \n50  The Australian Immigration Subreddit. This sub...  \n51                                                     \n52  Reddit's home for tax geeks and taxpayers! New...  \n53               A subreddit for Philippine taxation.  \n54  A place for discussions and queries on Indian ...  \n55  A community for Redditors who are tax professi...  \n56                                  Accidental racism  \n57  A subreddit for mocking reddit's large, vocal,...  \n58  Reddit's anti-racism community, a safe(r) spac...  \n59  A real discussion about racism is needed. Sinc...  \n\n\n\n\nCleaning Subreddit\n\n\ndf_raw = pd.read_csv(file_path_subreddit_raw)\n\n# Remove irrelevant subriddts\nsubreddits_to_remove = ['libertarianmeme','ConservativeMemes','ukpolitics','AustralianPolitics','CanadaPolitics','NewsOfTheStupid','NewSkaters','TradingView','remoteviewing','CrossView','lgbtmemes','climate_science','ImmigrationCanada','taxPH','IndiaTax','taxpros','AccidentalRacism']\ndf_filtered = df_raw[~df_raw['subreddit'].isin(subreddits_to_remove)]\n\n# Add valuable subriddts the search missed\nnew_rows = [\n    {\"keyword\": \"conservative\", \"subreddit\": \"AskTrumpSupporters\", \"subscribers\": 92000, \"description\": \"Q&A subreddit to understand Trump supporters, their views, and the reasons behind those views. Debates are discouraged.\"},\n    {\"keyword\": \"conservative\", \"subreddit\": \"AskConservatives\", \"subscribers\": 23000, \"description\": \"Welcome to r/AskConservatives! A sub to ask conservatives questions with the intent of better understanding Conservatism and conservative perspectives. The sub tends to have a focus on US politics, but we welcome all Canadian, UK, Aus, and European topics and users, as well as world politics in general. Open discussions are strongly encouraged. Please remember to keep things civil and respect others even when you disagree.\"},\n    {\"keyword\": \"libertarian\", \"subreddit\": \"AskLibertarians\", \"subscribers\": 12000, \"description\": \"A friendly place to learn about, critique, and question libertarians and their views. r/AskLibertarians is for any questions about the philosophy of libertarianism, libertarian movements and traditions, libertarian opinions on certain situations or current events, or anything else you feel is relevant. No question is too basic (or advanced!) to ask, so don't be shy. Subscribe :)\"},\n    {\"keyword\": \"politics\", \"subreddit\": \"moderatepolitics\", \"subscribers\": 298000, \"description\": \"Restore Sanity in Politics!This is NOT a politically moderate subreddit! It IS a political subreddit for moderately expressed opinions and civil discourse. If you are looking for civility, moderation and tolerance come on in!\"},\n]\n\ndf_new = pd.DataFrame(new_rows)\ndf_updated = pd.concat([df_filtered, df_new], ignore_index=True)\n\n# Remove subriddts that have less than 10000 subscribers\ndf_subreddit = df_updated[df_updated['subscribers'] &gt;= 10000]\nprint(f\"Total Subreddits: {len(df_subreddit)}\")\n\n# Save the final selected subriddts\nfile_path_subreddit = \"data/processed-data/subreddits.csv\"\ndf_subreddit.to_csv(file_path_subreddit, index=False)\ndf_subreddit.head(6)\n\nTotal Subreddits: 35\n\n\n\n\n\n\n\n\n\nkeyword\nsubreddit\nsubscribers\ndescription\n\n\n\n\n0\nlibertarian\nLibertarian\n503241\nWelcome to /r/Libertarian, a subreddit to disc...\n\n\n1\nlibertarian\nLibertarianLeft\n14930\nLiberty is the mother, not the daughter, of or...\n\n\n2\nlibertarian\nLibertarianSocialism\n13957\nA community for the discussion of news and lin...\n\n\n3\nconservative\nConservative\n1154369\nThe largest conservative subreddit.\\nhttps://d...\n\n\n4\nconservative\nconservatives\n108335\nConservatism (from conservare, \"to preserve\") ...\n\n\n5\nconservative\nconservativeterrorism\n85102\nDocumenting conservative terrorist actions and..."
  },
  {
    "objectID": "codes_model_nmf.html",
    "href": "codes_model_nmf.html",
    "title": "3. Modling – Multiple Class Prediction on Reddit Submission Topics",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nimport spacy\nimport gensim\nfrom gensim import corpora, models\nimport pyLDAvis.gensim_models\n\n\nPre-processing with SpaCy\n\n# Load data\nfile_path_text_clean = \"data/processed-data/text_clean.csv\"\ndf = pd.read_csv(file_path_text_clean)\n\n# Additional domain-specific stopwords\ndomain_stopwords = {\n    'trump', 'biden', 'republican', 'democrat', 'like', 'im', 'dont', 'people',\n    'think', 'know', 'would', 'said', 'one', 'year', 'state', 'time'\n}\n\n# Load SpaCy model for better text processing\nnlp = spacy.load('en_core_web_sm')\n\ndef improved_text_preprocessing(text):\n    # Handle non-string inputs\n    if not isinstance(text, str):\n        if pd.isna(text):  \n            return \"\"\n        text = str(text)  \n    \n    doc = nlp(text)\n    # Keep only nouns, adjectives, verbs, and adverbs\n    tokens = [token.lemma_.lower() for token in doc \n             if (token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']) \n             and (token.lemma_.lower() not in domain_stopwords)\n             and (len(token.lemma_) &gt; 2)]\n    return ' '.join(tokens)\n\n# Convert text column to string type and handle NaN values\ndf['text'] = df['text'].fillna(\"\").astype(str)\n\ndf['text'] = df['text'].apply(improved_text_preprocessing)\nsubmissions = df[df['type'].str.contains('submission')].copy()\n\n\n\nNMF Topic Modeling\n\ndef perform_nmf_analysis(texts, n_topics=5):\n    \"\"\"Perform NMF topic modeling\"\"\"\n    tfidf_vectorizer = TfidfVectorizer(\n        max_features=1000,\n        ngram_range=(1, 2)\n    )\n    tfidf = tfidf_vectorizer.fit_transform(texts)\n    \n    nmf_model = NMF(\n        n_components=n_topics,\n        random_state=42\n    )\n    nmf_topics = nmf_model.fit_transform(tfidf)\n    \n    return np.argmax(nmf_topics, axis=1), nmf_model, tfidf_vectorizer\n\n\n\nDBSCAN Clustering Modeling\n\ndef perform_dbscan_validation(texts, tfidf_vectorizer):\n    print(\"\\nPerforming DBSCAN validation...\")\n    \n    # Create TF-IDF vectors\n    tfidf = tfidf_vectorizer.transform(texts)\n    \n    # Scale the features\n    scaler = StandardScaler(with_mean=False)\n    scaled_tfidf = scaler.fit_transform(tfidf)\n    \n    # Apply DBSCAN\n    dbscan = DBSCAN(eps=0.5, min_samples=5)\n    dbscan_labels = dbscan.fit_predict(scaled_tfidf)\n    \n    # Count number of clusters found\n    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n    print(f\"DBSCAN found {n_clusters} clusters\")\n    print(f\"Number of samples in each cluster: {pd.Series(dbscan_labels).value_counts().sort_index()}\")\n    \n    # K-distance plot (distance to the 5th nearest neighbor)\n    neigh = NearestNeighbors(n_neighbors=5)\n    neigh.fit(scaled_tfidf)\n    distances, indices = neigh.kneighbors(scaled_tfidf)\n    \n    # Plot k-distance graph (distances to 5th nearest neighbor)\n    distances = np.sort(distances[:, -1], axis=0)\n    plt.plot(distances)\n    plt.title('k-distance plot')\n    plt.xlabel('Points sorted by distance')\n    plt.ylabel('Distance to 5th nearest neighbor')\n    plt.show()\n    \n    return dbscan_labels\n\n\n\nLDA Topic Modeling\n\n# def perform_lda_analysis(texts, n_topics=5):\n#     # Create dictionary and corpus\n#     texts_tokenized = [text.split() for text in texts]\n#     dictionary = corpora.Dictionary(texts_tokenized)\n#     corpus = [dictionary.doc2bow(text) for text in texts_tokenized]\n    \n#     # Train LDA model\n#     lda_model = models.LdaModel(\n#         corpus,\n#         num_topics=n_topics,\n#         id2word=dictionary,\n#         passes=15,\n#         random_state=42\n#     )\n    \n#     # Create visualization\n#     vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n#     pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n    \n#     # Get topic assignments\n#     topic_assignments = []\n#     for doc in corpus:\n#         topics = lda_model.get_document_topics(doc)\n#         main_topic = max(topics, key=lambda x: x[1])[0]\n#         topic_assignments.append(main_topic)\n        \n#     return topic_assignments, lda_model\n\n\n\nResults\n\n# Apply all methods\nprint(\"Performing topic analysis...\")\n\n# # 1. LDA Analysis\n# lda_topics, lda_model = perform_lda_analysis(submissions['text'])\n# submissions['lda_topic'] = lda_topics\n# print(\"\\nLDA Topics:\")\n# for idx in range(lda_model.num_topics):\n#     print(f\"Topic {idx}:\", end=' ')\n#     terms = lda_model.show_topic(idx, 10)\n#     print(', '.join([term for term, _ in terms]))\n\n# 2. NMF Analysis -- best\nnmf_topics, nmf_model, tfidf_vectorizer = perform_nmf_analysis(submissions['text'])\nsubmissions['nmf_topic'] = nmf_topics\n# Function to display top terms for each topic\ndef display_top_terms(model, feature_names, n_top_words=10):\n    for topic_idx, topic in enumerate(model.components_):\n        top_terms = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n        print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n\n# Display results\nprint(\"\\nNMF Topics:\")\ndisplay_top_terms(nmf_model, tfidf_vectorizer.get_feature_names_out())\n\n# 3. DBSCAN Clustering\nsubmissions['dbscan_cluster'] = perform_dbscan_validation(submissions['text'], tfidf_vectorizer)\n\n# Compare NMF and DBSCAN results\nprint(\"\\nComparison of NMF topics and DBSCAN clusters:\")\nprint(pd.crosstab(submissions['nmf_topic'], submissions['dbscan_cluster']))\n\nPerforming topic analysis...\n\nNMF Topics:\nTopic 0: say, abortion, feel, get, want, right, make, woman, even, life\nTopic 1: gun, rifle, reddit, gun owner, owner, assault, assault rifle, law, health, couple\nTopic 2: tax, pay, pay tax, work, income, system, low, call, federal, amount\nTopic 3: climate, change, climate change, call, point, news, article, insurance, meat, help\nTopic 4: vote, election, conservative, end, communist, worker, last, voting, socialist, democratic\n\nPerforming DBSCAN validation...\nDBSCAN found 1 clusters\nNumber of samples in each cluster: -1    648\n 0     29\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\n\nComparison of NMF topics and DBSCAN clusters:\ndbscan_cluster   -1   0\nnmf_topic              \n0               427  29\n1                61   0\n2                45   0\n3                52   0\n4                63   0\n\n\n\n\nAssigning Topics\n\n# Create simplified topic labels (select the top term)\ntopic_labels = {\n    0: \"abortion\",  \n    1: \"guns\",      \n    2: \"tax\",       \n    3: \"climate\",   \n    4: \"politics\"   \n}\n\n# Add topic labels to submissions\nsubmissions['nmf_topic'] = submissions['nmf_topic'].map(topic_labels)\n\n\n# Create a new column 'nmf_topic' to store the topic label for each row\ndf['nmf_topic'] = None\n\n# Variables to store the current submission ID and its topic\ncurrent_submission_id = None\ncurrent_topic = None\n\n# Iterate through the dataset and assign the topic of the parent submission to each comment\nfor idx, row in df.iterrows():\n    if 'submission' in row['type']:  # If the current row is a submission\n        # Get the current submission's ID and its topic\n        current_submission_id = row['id']\n        current_topic = submissions[submissions['id'] == current_submission_id]['nmf_topic'].values\n        if current_topic.size &gt; 0:\n            current_topic = current_topic[0]\n        df.at[idx, 'nmf_topic'] = current_topic  \n    # If the row is a comment, assign the parent submission's topic\n    if 'comment' in row['type'] and current_submission_id is not None:\n        df.at[idx, 'nmf_topic'] = current_topic\n\n\n# Save results\ndf_text_topic = df\n\nfile_path_text_topic = \"data/processed-data/text_topic.csv\"\ndf_text_topic.to_csv(file_path_text_topic, index=False)\n\nprint(f\"\\nModeling complete. Results saved to {file_path_text_topic}\")\ndf_text_topic.head(6)\n\n# Display sample results\nprint(\"\\nSample results:\")\ndf_text_topic.head(6)\n\n\nModeling complete. Results saved to data/processed-data/text_topic.csv\n\nSample results:\n\n\n\n\n\n\n\n\n\nsubreddit\nid\ntype\ndepth\nscore\ntime\ntext\nnmf_topic\n\n\n\n\n0\nLibertarian\n1hf706u\nsubmission_hot\n0\n100\n2024/12\nroad serfdom new libertarian economic let borr...\nabortion\n\n\n1\nLibertarian\nm29svuv\ncomment\n1\n1\n2024/12\nfredrich bastiat also good actually make funct...\nabortion\n\n\n2\nLibertarian\nm2a5co6\ncomment\n2\n1\n2024/12\nlibertarian exclusively anarchist\nabortion\n\n\n3\nLibertarian\nm2a694w\ncomment\n3\n1\n2024/12\ntrue socialist communist always say fix ideolo...\nabortion\n\n\n4\nLibertarian\nm29bopi\ncomment\n1\n7\n2024/12\ngood favorite always recommend start revolutio...\nabortion\n\n\n5\nLibertarian\nm2a1cm0\ncomment\n1\n1\n2024/12\nsalma write book become movie star\nabortion"
  },
  {
    "objectID": "codes_model_dt.html",
    "href": "codes_model_dt.html",
    "title": "4. Modeling – Binary Class Prediction on Text Political Lean",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, validation_curve\nimport matplotlib.pyplot as plt\n\n\nTF-IDF Vectorizer\n\n# Load training data\nfile_path_train_lean = \"data/processed-data/train_lean.csv\"\ntrain_data = pd.read_csv(file_path_train_lean)\ntrain_data = train_data.dropna(subset=['Text', 'Political Lean'])  # Drop rows with NaN in targetted columns\n\n# Select features and target\nX_train_text = train_data['Text']\ny_train = train_data['Political Lean']\n\n# Text vectorization (TF-IDF)\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  \nX_train = tfidf_vectorizer.fit_transform(X_train_text)\n\n\n\nCross Validation\n\n# Define hyperparameter grid for Decision Tree\nparam_grid = {\n    'max_depth': [10, 15, 20, 25, 30, 50, None],  # Adjust depth of the tree\n    'min_samples_split': [2, 5, 10, 20, 50],  # Minimum samples required to split a node\n    'min_samples_leaf': [1, 2, 5, 10, 20],  # Minimum samples required at a leaf node\n    'criterion': ['gini', 'entropy']  # Criterion for splitting\n}\n\n# Initialize Decision Tree classifier\nclf = DecisionTreeClassifier(random_state=5000)\n\n# Perform GridSearchCV for hyperparameter tuning\ngrid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Output best parameters from GridSearchCV\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best cross-validation score: {:.4f}\".format(grid_search.best_score_))\n\n# Perform cross-validation on the best estimator found by GridSearchCV\ncv_scores = cross_val_score(grid_search.best_estimator_, X_train, y_train, cv=5, scoring='accuracy')\n\nprint(\"Cross Validation Results:\")\nprint(f\"Accuracy Scores: {cv_scores}\")\nprint(f\"Mean Accuracy: {cv_scores.mean():.4f}\")\nprint(f\"Standard Deviation: {cv_scores.std():.4f}\")\n\n# Train the best model on the full training set\nbest_clf = grid_search.best_estimator_\nbest_clf.fit(X_train, y_train)\n\nBest parameters found:  {'criterion': 'gini', 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2}\nBest cross-validation score: 0.6540\nCross Validation Results:\nAccuracy Scores: [0.63580247 0.65843621 0.64197531 0.65979381 0.6742268 ]\nMean Accuracy: 0.6540\nStandard Deviation: 0.0137\n\n\nDecisionTreeClassifier(max_depth=15, random_state=5000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=15, random_state=5000) \n\n\n\n\nVisualization and Evaluation\n\n# Visualize the trained decision tree\nplt.figure(figsize=(20, 10))\nplot_tree(best_clf, feature_names=tfidf_vectorizer.get_feature_names_out(), class_names=best_clf.classes_, filled=True)\nplt.title(\"Optimized Decision Tree Visualization\")\nplt.show()\n\n# Model evaluation (on training data)\ny_train_pred = best_clf.predict(X_train)\nprint(\"Training Set Evaluation:\")\nprint(classification_report(y_train, y_train_pred))\nprint(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n\n\n\n\n\n\n\n\nTraining Set Evaluation:\n              precision    recall  f1-score   support\n\nConservative       0.94      0.48      0.63       957\n     Liberal       0.74      0.98      0.84      1471\n\n    accuracy                           0.78      2428\n   macro avg       0.84      0.73      0.74      2428\nweighted avg       0.82      0.78      0.76      2428\n\nTraining Accuracy: 0.7821252059308073\n\n\n\n\nPrediction\n\n# Load test data\nfile_path_text_topic = \"data/processed-data/text_topic.csv\"\ntest_data = pd.read_csv(file_path_text_topic)\ntest_data = test_data.dropna(subset=['text'])\n\n# Use the same vectorizer\nX_test_text = test_data['text']\nX_test = tfidf_vectorizer.transform(X_test_text)\n\n# Predict on test data\npredictions = best_clf.predict(X_test)\n\n# Add predictions to the test set\ntest_data['dt_lean'] = predictions\n\n# Save predictions\nfile_path_text_lean = \"data/processed-data/text_lean.csv\"\ndf_text_lean = test_data\ndf_text_lean.to_csv(file_path_text_lean, index=False)\n\nprint(f\"Modeling complete. Results saved to {file_path_text_lean}\")\ndf_text_lean.head(6)\n\nModeling complete. Results saved to data/processed-data/text_lean.csv\n\n\n\n\n\n\n\n\n\nsubreddit\nid\ntype\ndepth\nscore\ntime\ntext\nnmf_topic\ndt_lean\n\n\n\n\n0\nLibertarian\n1hf706u\nsubmission_hot\n0\n100\n2024/12\nroad serfdom new libertarian economic let borr...\nabortion\nConservative\n\n\n1\nLibertarian\nm29svuv\ncomment\n1\n1\n2024/12\nfredrich bastiat also good actually make funct...\nabortion\nLiberal\n\n\n2\nLibertarian\nm2a5co6\ncomment\n2\n1\n2024/12\nlibertarian exclusively anarchist\nabortion\nConservative\n\n\n3\nLibertarian\nm2a694w\ncomment\n3\n1\n2024/12\ntrue socialist communist always say fix ideolo...\nabortion\nConservative\n\n\n4\nLibertarian\nm29bopi\ncomment\n1\n7\n2024/12\ngood favorite always recommend start revolutio...\nabortion\nLiberal\n\n\n5\nLibertarian\nm2a1cm0\ncomment\n1\n1\n2024/12\nsalma write book become movie star\nabortion\nLiberal"
  },
  {
    "objectID": "codes_model_rf.html",
    "href": "codes_model_rf.html",
    "title": "5. Modeling – Regression Prediction on Text Toxicity",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom tqdm import tqdm\nfrom scipy.sparse import issparse\n\n\nTF-IDF Vectorizer\n\n# Load and preprocess training data\nfile_path_train_toxicity = \"data/processed-data/train_toxicity.csv\"\ntrain_data = pd.read_csv(file_path_train_toxicity)\ntrain_data = train_data.dropna(subset=['comment_text', 'target'])  # Drop rows with NaN in targetted columns\n\n# Split features and target\nX_train_text = train_data['comment_text']  \ny_train = train_data['target']            \n\n# Text vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\nX_train = tfidf_vectorizer.fit_transform(X_train_text)\n\n# Converting to dense matrix will lead the running time to even rise, so sparese matrix kept \n# print(issparse(X_train))\n# X_train = X_train.toarray()\n# print(issparse(X_train))\n\n\n\nCross Validation\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    'n_estimators': [100, 200, 300, 350, 500],\n    'max_depth': [10, 15, 20, 25, 50, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 5, 10],\n    'max_features': ['sqrt', 'log2']\n}\n\n# Initialize and train Random Forest Regressor\nrf = RandomForestRegressor(random_state=5000)  \n\n# Perform GridSearchCV\ngrid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\ngrid_search.fit(X_train, y_train)\n\n# Output the best parameters and score from GridSearchCV\nprint(\"Best parameters found: \", grid_search.best_params_)\nprint(\"Best cross-validation MSE: \", grid_search.best_score_)\n\n# Use the best estimator from GridSearchCV\nbest_rf = grid_search.best_estimator_\n\n# Perform cross-validation with progress tracking\nprint(\"Performing Cross Validation...\")\ncv_scores = []\nfor train_index, test_index in tqdm(KFold(n_splits=5, shuffle=True, random_state=5000).split(X_train), desc=\"Cross Validation Progress\"):\n    # Train-test split\n    X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n    y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n    \n    # Train and evaluate\n    best_rf.fit(X_train_fold, y_train_fold)\n    fold_score = mean_squared_error(y_test_fold, best_rf.predict(X_test_fold))\n    cv_scores.append(fold_score)\n\n\n# Output cross-validation results\nprint(\"\\n--- Cross-validation Results ---\")\nprint(f\"Cross-validation MSE (per fold): {cv_scores}\")\nprint(f\"Mean Cross-validation MSE: {np.mean(cv_scores):.4f}\")\nprint(f\"Standard Deviation of Cross-validation MSE: {np.std(cv_scores):.4f}\")\n\nFitting 3 folds for each of 540 candidates, totalling 1620 fits\nBest parameters found:  {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\nBest cross-validation MSE:  -0.02032021236485095\nPerforming Cross Validation...\n\n\nCross Validation Progress: 5it [06:14, 74.95s/it]\n\n\n\n--- Cross-validation Results ---\nCross-validation MSE (per fold): [0.019495475718510954, 0.02001842412618114, 0.020684961097825923, 0.019259745203942604, 0.018090267311264653]\nMean Cross-validation MSE: 0.0195\nStandard Deviation of Cross-validation MSE: 0.0009\n\n\n\n\n\n\n\nTraining and Evaluation\n\n# Train the model on the entire training data\nbest_rf.fit(X_train, y_train)\n\n# Model evaluation (on training data)\ny_train_pred = best_rf.predict(X_train)\nmse = mean_squared_error(y_train, y_train_pred)\nmae = mean_absolute_error(y_train, y_train_pred)\nr2 = r2_score(y_train, y_train_pred)\nevs = explained_variance_score(y_train, y_train_pred)\n\nprint(\"\\n--- Model Evaluation on Training Data ---\")\nprint(f\"Mean Squared Error (MSE): {mse:.4f}\")\nprint(f\"Mean Absolute Error (MAE): {mae:.4f}\")\nprint(f\"R-squared (R2): {r2:.4f}\")\nprint(f\"Explained Variance Score: {evs:.4f}\")\n\n\n--- Model Evaluation on Training Data ---\nMean Squared Error (MSE): 0.0029\nMean Absolute Error (MAE): 0.0326\nR-squared (R2): 0.9088\nExplained Variance Score: 0.9089\n\n\n\n\nPrediction\n\n# Load test data\nfile_path_text_lean = \"data/processed-data/text_lean.csv\"\ntest_data = pd.read_csv(file_path_text_lean)\ntest_data = test_data.dropna(subset=['text'])\n\n# Use the same vectorizer\nX_test_text = test_data['text']\nX_test = tfidf_vectorizer.transform(X_test_text)\n\n# Predict toxicity values on test data\ntest_data['rf_toxicity'] = best_rf.predict(X_test)\ndf_text_toxicity = test_data\n\n# Save predictions\nfile_path_text_toxicity = \"data/processed-data/text_toxicity.csv\"\ndf_text_toxicity.to_csv(file_path_text_toxicity, index=False)\nprint(f\"Modling complete. Results saved to {file_path_text_toxicity}\")\ndf_text_toxicity.head(13)\n\nModling complete. Results saved to data/processed-data/text_toxicity.csv\n\n\n\n\n\n\n\n\n\nsubreddit\nid\ntype\ndepth\nscore\ntime\ntext\nnmf_topic\ndt_lean\nrf_toxicity\n\n\n\n\n0\nLibertarian\n1hf706u\nsubmission_hot\n0\n100\n2024/12\nroad serfdom new libertarian economic let borr...\nabortion\nConservative\n0.041974\n\n\n1\nLibertarian\nm29svuv\ncomment\n1\n1\n2024/12\nfredrich bastiat also good actually make funct...\nabortion\nLiberal\n0.148867\n\n\n2\nLibertarian\nm2a5co6\ncomment\n2\n1\n2024/12\nlibertarian exclusively anarchist\nabortion\nConservative\n0.088044\n\n\n3\nLibertarian\nm2a694w\ncomment\n3\n1\n2024/12\ntrue socialist communist always say fix ideolo...\nabortion\nConservative\n0.059692\n\n\n4\nLibertarian\nm29bopi\ncomment\n1\n7\n2024/12\ngood favorite always recommend start revolutio...\nabortion\nLiberal\n0.013902\n\n\n5\nLibertarian\nm2a1cm0\ncomment\n1\n1\n2024/12\nsalma write book become movie star\nabortion\nLiberal\n0.054371\n\n\n6\nLibertarian\nm2abas1\ncomment\n1\n1\n2024/12\nsure really good place start recommend economi...\nabortion\nLiberal\n0.009030\n\n\n7\nLibertarian\nm29pifi\ncomment\n1\n1\n2024/12\nclassic\nabortion\nLiberal\n0.053227\n\n\n8\nLibertarian\n1hf07so\nsubmission_hot\n0\n233\n2024/12\npay income tax due norwegian wealth tax fuck\ntax\nLiberal\n0.783270\n\n\n9\nLibertarian\nm27qglj\ncomment\n1\n93\n2024/12\nfree\ntax\nLiberal\n0.000095\n\n\n10\nLibertarian\nm27xku7\ncomment\n2\n59\n2024/12\nfree\ntax\nLiberal\n0.000095\n\n\n11\nLibertarian\nm292tkk\ncomment\n2\n14\n2024/12\nhate idea free country old side share health p...\ntax\nConservative\n0.053861\n\n\n12\nLibertarian\nm29s3ia\ncomment\n3\n1\n2024/12\nyet meet canadian person diagnose cancer good ...\ntax\nLiberal\n0.266743"
  },
  {
    "objectID": "codes_text_collection.html",
    "href": "codes_text_collection.html",
    "title": "Initialize",
    "section": "",
    "text": "import csv\nimport json\nimport praw\nimport pandas as pd\nfrom datetime import datetime, timezone\n\nclient_id = 'XqvMFAQqrS1xWiUpCcbSOg'\nclient_secret = '1WDnClQ3eo8Luzug2FSmZ6x9Aj1Ytg'\nuser_agent = 'ios:MyRedditApp:v1.0 (by /u/sk_shuanq)'\n\nreddit = praw.Reddit(\n    client_id=client_id,\n    client_secret=client_secret,\n    user_agent=user_agent\n)\n\n# Test if connected\nprint(reddit.read_only)  # Expected True\n\ntry:\n    # Test with subreddit: python\n    subreddit = reddit.subreddit('python')\n    print(f\"Successfully connected! Subreddit title: {subreddit.title}\")\n    print(f\"Read-only mode: {reddit.read_only}\")  # True Expected\nexcept Exception as e:\n    print(f\"Connection failed: {e}\")\n\nTrue\nSuccessfully connected! Subreddit title: Python\nRead-only mode: True"
  },
  {
    "objectID": "codes_text_collection.html#text-submissioncomments-collection",
    "href": "codes_text_collection.html#text-submissioncomments-collection",
    "title": "Initialize",
    "section": "2. Text (Submission&Comments) Collection",
    "text": "2. Text (Submission&Comments) Collection\n\nfile_path_subreddit = \"data/processed-data/subreddits.csv\"\ndf_subreddit = pd.read_csv(file_path_subreddit)\nsubreddit_list = df_subreddit['subreddit'].tolist()\n\n# Storage for results\nresults = []\n\n# Counters for submission and comments\nsubmission_count = 0\ncomment_count = 0\n\n# Function to convert UTC timestamp to readable format\ndef convert_timestamp(utc_timestamp):\n    return datetime.fromtimestamp(utc_timestamp, timezone.utc).strftime('%Y/%m')\n\n# Iterate over each subreddit\nfor subreddit_name in subreddit_list:\n    print(f\"Fetching data for subreddit: {subreddit_name}\")\n    try:\n        subreddit = reddit.subreddit(subreddit_name)\n        \n        # Store post IDs to avoid duplicates\n        seen_post_ids = set()\n        \n        # Submission Collection Part 1: Get top 10 posts by hot ranking\n        for submission in subreddit.hot(limit=10):\n            \n            # Skip repeat submission\n            if submission.id in seen_post_ids:\n                continue\n\n            # Skip posts with the \"meme\" flair\n            if submission.link_flair_text and \"meme\" in submission.link_flair_text.lower():\n                print(f\"Skipping post with 'meme' flair: {submission.title}\")\n                continue\n\n            submission_data = {\n                \"subreddit\": subreddit_name,\n                \"post_id\": submission.id,\n                \"title\": submission.title,\n                \"text\": submission.selftext,\n                \"score\": submission.score,\n                \"created_utc\": convert_timestamp(submission.created_utc),\n                \"sort_type\": \"hot\",\n                \"comments\": []\n            }\n\n            seen_post_ids.add(submission.id)\n            submission_count += 1\n\n            # Process comments for this submission\n            submission.comments.replace_more(limit=0)  \n            all_comments = submission.comments.list()\n            \n            # Sort comments by number of replies\n            comments_by_replies = sorted(\n                all_comments,\n                key=lambda x: len(x.replies) if hasattr(x, 'replies') else 0,\n                reverse=True\n            )[:10]\n            \n            # Process comments and add indentation\n            def process_comments(comments, depth=0):\n                global comment_count\n                processed_comments = []\n                \n                for comment in comments:\n                    # indent = \"  \" * depth  \n                    processed_comment = {\n                        \"comment_id\": comment.id,\n                        \"body\": comment.body,\n                        \"score\": comment.score,\n                        \"depth\": depth,\n                        \"num_replies\": len(comment.replies) if hasattr(comment, 'replies') else 0,\n                        \"created_utc\": convert_timestamp(comment.created_utc)  \n                    }\n                    \n                    comment_count += 1\n                    processed_comments.append(processed_comment)\n                    \n                    # Process replies if they exist\n                    if hasattr(comment, 'replies') and len(comment.replies) &gt; 0:\n                        replies = list(comment.replies)[:10]  # Limit replies to 10 per comment\n                        processed_comments.extend(process_comments(replies, depth + 1))\n                \n                return processed_comments\n\n            # Process and store comments for the current submission\n            submission_data[\"comments\"] = process_comments(comments_by_replies)\n            results.append(submission_data)\n\n\n        # Submission Collection Part 2: Get top 10 posts by controversial ranking\n        for submission in subreddit.controversial(limit=10):\n            if submission.id in seen_post_ids:\n                continue\n                \n            # Skip posts with the \"meme\" flair\n            if submission.link_flair_text and \"meme\" in submission.link_flair_text.lower():\n                print(f\"Skipping post with 'meme' flair: {submission.title}\")\n                continue\n            \n            submission_data = {\n                \"subreddit\": subreddit_name,\n                \"post_id\": submission.id,\n                \"title\": submission.title,\n                \"text\": submission.selftext,\n                \"score\": submission.score,\n                \"created_utc\": convert_timestamp(submission.created_utc),  \n                \"sort_type\": \"controversial\",  \n                \"comments\": []\n            }\n            \n            seen_post_ids.add(submission.id)\n            submission_count += 1\n\n            # Process comments for this submission\n            submission.comments.replace_more(limit=0)\n            all_comments = submission.comments.list()\n            \n            # Sort comments by number of replies\n            comments_by_replies = sorted(\n                all_comments,\n                key=lambda x: len(x.replies) if hasattr(x, 'replies') else 0,\n                reverse=True\n            )[:10]\n            \n            # Process and store comments using the same process_comments function\n            submission_data[\"comments\"] = process_comments(comments_by_replies)\n            results.append(submission_data)\n\n    except Exception as e:\n        print(f\"Error fetching data for subreddit {subreddit_name}: {e}\")\n        continue\n\n# Print summary of fetched data\nprint(f\"Total posts fetched: {submission_count}\")\nprint(f\"Total comments fetched: {comment_count}\")\n\nFetching data for subreddit: Libertarian\nSkipping post with 'meme' flair: Statism in a Nutshell\nSkipping post with 'meme' flair: Welcome to the Twilight Zone\nSkipping post with 'meme' flair: How a Modern Day Mengele; became hugely wealthy promoting the NWO Agenda\nFetching data for subreddit: LibertarianLeft\nFetching data for subreddit: LibertarianSocialism\nFetching data for subreddit: Conservative\nFetching data for subreddit: conservatives\nFetching data for subreddit: conservativeterrorism\nFetching data for subreddit: politics\nFetching data for subreddit: Republican\nFetching data for subreddit: republicans\nFetching data for subreddit: RepublicanValues\nFetching data for subreddit: RepublicanPedophiles\nFetching data for subreddit: democrats\nSkipping post with 'meme' flair: Forget \"of the people, by the people.\" America is now of the wealthy, by the corporations.\nSkipping post with 'meme' flair: No Man Should View Trump as a Role Model\nFetching data for subreddit: DemocraticSocialism\nFetching data for subreddit: worldnews\nFetching data for subreddit: news\nFetching data for subreddit: changemyview\nFetching data for subreddit: Firearms\nFetching data for subreddit: liberalgunowners\nFetching data for subreddit: GunsAreCool\nFetching data for subreddit: guncontrol\nSkipping post with 'meme' flair: 🇦🇺 Never again, thankfully. 🇦🇺\nSkipping post with 'meme' flair: How can you distinguish someone who has a gun for self-defense from someone who has it gun in hopes they can use it? You can’t.\nSkipping post with 'meme' flair: Photos from the March For Our Lives in Washington, DC on Saturday\nFetching data for subreddit: lgbt\nFetching data for subreddit: LGBTeens\nFetching data for subreddit: LGBTnews\nFetching data for subreddit: abortion\nFetching data for subreddit: Abortiondebate\nFetching data for subreddit: climatechange\nFetching data for subreddit: ClimateOffensive\nFetching data for subreddit: immigration\nFetching data for subreddit: tax\nFetching data for subreddit: FragileWhiteRedditor\nFetching data for subreddit: racism\nFetching data for subreddit: AskTrumpSupporters\nFetching data for subreddit: AskConservatives\nFetching data for subreddit: AskLibertarians\nFetching data for subreddit: moderatepolitics\nTotal posts fetched: 692\nTotal comments fetched: 59032\n\n\n\n# Save results to a JSON file\nfile_path_text_raw = \"data/raw-data/text_raw.json\"\nwith open(file_path_text_raw, \"w\", encoding=\"utf-8\") as f:\n    json.dump(results, f, ensure_ascii=False, indent=4)\n\nprint(f\"Data collection complete. Results saved to {file_path_text_raw}.\")\n\nData collection complete. Results saved to data/raw-data/text_raw.json.\n\n\n\nFile Format Converting\n\n# Read the json data\ndef convert_reddit_data(json_input_path, csv_output_path):\n    with open(json_input_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n\n    # Lists to store the processed data\n    processed_data = []\n\n    # Process each submission and its comments\n    for submission in data:\n        submission_text = submission['title']\n        if submission['text']:  # Combine text content if it exists\n            submission_text += \" \" + submission['text']\n            \n        # Add submission data\n        processed_data.append({\n            'subreddit': submission['subreddit'],\n            'id': submission['post_id'],\n            'type': f\"submission_{submission['sort_type']}\",  # submission_hot or submission_con\n            'depth': 0,  # submissions are depth 0\n            'score': submission['score'],\n            'time': submission['created_utc'],\n            'text': submission_text\n        })\n        \n        # Process comments\n        for comment in submission['comments']:\n            processed_data.append({\n                'subreddit': submission['subreddit'],\n                'id': comment['comment_id'],\n                'type': 'comment',\n                'depth': comment['depth'] + 1,  # increment depth by 1\n                'score': comment['score'],\n                'time': comment['created_utc'],\n                'text': comment['body']\n            })\n\n    # Convert to DataFrame\n    df = pd.DataFrame(processed_data)\n    \n    # Save to CSV\n    df.to_csv(csv_output_path, index=False)\n    \n    # Print some basic statistics\n    print(\"\\nFile convert complete. Results saved to data/processed-data/text_inital.json.\")\n    print(f\"Total rows: {len(df)}\")\n    print(\"\\nDistribution by type:\")\n    print(df['type'].value_counts())\n    print(\"\\nDepth distribution:\")\n    print(df['depth'].value_counts().sort_index())\n\n    return df\n\n# Execute the conversion\nfile_path_text_raw = \"data/raw-data/text_raw.json\"\nfile_path_text_initial = \"data/processed-data/text_initial.csv\"  \n\ndf_text_initial = convert_reddit_data(file_path_text_raw, file_path_text_initial)\ndf_text_initial.head(6)\n\n\nFile convert complete. Results saved to data/processed-data/text_inital.json.\nTotal rows: 59724\n\nDistribution by type:\ntype\ncomment                     59032\nsubmission_hot                347\nsubmission_controversial      345\nName: count, dtype: int64\n\nDepth distribution:\ndepth\n0       692\n1      5573\n2     13747\n3     11078\n4      9039\n5      6583\n6      4939\n7      3411\n8      2353\n9      1495\n10      814\nName: count, dtype: int64\n\n\n\n\n\n\n\n\n\nsubreddit\nid\ntype\ndepth\nscore\ntime\ntext\n\n\n\n\n0\nLibertarian\n1hf706u\nsubmission_hot\n0\n100\n2024/12\nThe road to serfdom New to libertarian economi...\n\n\n1\nLibertarian\nm29svuv\ncomment\n1\n1\n2024/12\nAnything by Fredrich Bastiat is also good but ...\n\n\n2\nLibertarian\nm2a5co6\ncomment\n2\n1\n2024/12\nLibertarians aren't exclusively anarchists.\n\n\n3\nLibertarian\nm2a694w\ncomment\n3\n1\n2024/12\nTrue but like socialists and communists always...\n\n\n4\nLibertarian\nm2a5co6\ncomment\n1\n1\n2024/12\nLibertarians aren't exclusively anarchists.\n\n\n5\nLibertarian\nm2a694w\ncomment\n2\n1\n2024/12\nTrue but like socialists and communists always...\n\n\n\n\n\n\n\n\n\nText Cleaning\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport emoji\n\n# Download NLTK stopwords and WordNet data\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nstop_words = set(stopwords.words('english'))\nlemmatizer = WordNetLemmatizer()\n\n# Function to clean text\ndef clean_text(text):\n    # Remove extra spaces\n    text = text.strip()\n    \n    # Remove newline characters and extra spaces\n    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace characters (including newline, tabs) with a single space\n    \n    # Remove meaningless characters, such as special symbols \n    text = re.sub(r'[^\\w\\s,.\\-]', '', text)  # Keep letters, numbers, spaces, commas, periods, and hyphens\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n    text = re.sub(r'@\\S+', '', text)  # Remove @username mentions\n    text = re.sub(r'#[\\w]+', '', text)  # Remove all hashtags (e.g., #aiart)\n    text = re.sub(r'-', '', text)  # Remove hyphens\n    \n    # Remove emojis\n    text = emoji.replace_emoji(text, replace='')  \n    \n    # Convert the text to lowercase\n    text = text.lower()\n\n    # Remove double quotes\n    text = text.replace('\"', '')  \n\n    if text.startswith('\"') and text.endswith('\"'):\n        text = text[1:-1]\n\n    return text\n\ndef remove_non_english_words(text):\n    # English words only\n    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n    return ' '.join(english_words)\n\ndef process_text(text):\n    # Tokenize the text into words\n    words = text.split()\n    # Remove stop words and apply lemmatization\n    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n    # Join the processed words back into a string\n    return ' '.join(processed_words)\n\n# Clean the text of each row in the 'text' column\ndf_text_initial['text'] = df_text_initial['text'].apply(lambda x: clean_text(x))  # Clean the 'text' field\ndf_text_initial['text'] = df_text_initial['text'].apply(lambda x: remove_non_english_words(x))  # Keep only English words\ndf_text_initial['text'] = df_text_initial['text'].apply(lambda x: process_text(x))  # Remove stop words and apply lemmatization\n\n# Drop duplicate rows based on the 'text' column\ndf_text_clean = df_text_initial.drop_duplicates(subset=['text'])\n\n# Output the cleaned DataFrame to a CSV file\nfile_path_text_clean = \"data/processed-data/text_clean.csv\"\ndf_text_clean.to_csv(file_path_text_clean, index=False, encoding='utf-8')\n\nprint(f\"Data cleaning complete. Results saved to {file_path_text_clean}\")\ndf_text_clean.head(6)\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/guochenxi/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/guochenxi/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nData cleaning complete. Results saved to data/processed-data/text_clean.csv\n\n\n\n\n\n\n\n\n\nsubreddit\nid\ntype\ndepth\nscore\ntime\ntext\n\n\n\n\n0\nLibertarian\n1hf706u\nsubmission_hot\n0\n100\n2024/12\nroad serfdom new libertarian economics dad let...\n\n\n1\nLibertarian\nm29svuv\ncomment\n1\n1\n2024/12\nanything fredrich bastiat also good think stat...\n\n\n2\nLibertarian\nm2a5co6\ncomment\n2\n1\n2024/12\nlibertarian arent exclusively anarchist\n\n\n3\nLibertarian\nm2a694w\ncomment\n3\n1\n2024/12\ntrue like socialist communist always saying fi...\n\n\n6\nLibertarian\nm29bopi\ncomment\n1\n7\n2024/12\nyeah good favorite always recommend get starte...\n\n\n7\nLibertarian\nm2a1cm0\ncomment\n1\n1\n2024/12\nsalma write book becoming movie star"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Step 1: Send a request to Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\nresponse = requests.get(url)\n\n# Step 2: Parse the page content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Step 3: Find the table containing the data (usually the first table for such lists)\ntable = soup.find('table', {'class': 'wikitable'})\n\n# Step 4: Extract data from the table rows\ncountries = []\npopulations = []\n\n# Iterate over the table rows\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    if len(cells) &gt; 1:\n        country = cells[1].text.strip()  # The country name is in the second column\n        population = cells[2].text.strip()  # The population is in the third column\n        countries.append(country)\n        populations.append(population)\n\n# Step 5: Create a DataFrame to store the results\ndata = pd.DataFrame({\n    'Country': countries,\n    'Population': populations\n})\n\n# Display the scraped data\nprint(data)\n\n# Optionally save to CSV\ndata.to_csv('../../data/raw-data/countries_population.csv', index=False)\n\n                                 Country     Population\n0                                  World  8,119,000,000\n1                                  China  1,409,670,000\n2                          1,404,910,000          17.3%\n3                          United States    335,893,238\n4                              Indonesia    281,603,800\n..                                   ...            ...\n235                   Niue (New Zealand)          1,681\n236                Tokelau (New Zealand)          1,647\n237                         Vatican City            764\n238  Cocos (Keeling) Islands (Australia)            593\n239                Pitcairn Islands (UK)             35\n\n[240 rows x 2 columns]"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Final Report",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website.\nThis report is designed for a non-technical audience (e.g., the general public, executives, marketing teams, or clients), focusing on high-level insights, actionable results, and visualizations to convey the impact without requiring technical knowledge. The goal is to highlight how a model affects business strategy or revenue without diving into complex methods.\n\n\n\nClear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content.\n\n\n\n\nThese are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights.\n\n\n\n\n\n\nSimplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "This page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools were used to brainstorm ideas and provide feedback and refine the project plan."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation"
  }
]