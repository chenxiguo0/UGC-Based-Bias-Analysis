{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include cleaning.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE: This code is supposed to be run within the same .ipynb file as the data collection codes.\n",
    "\n",
    "# Read the json data\n",
    "def convert_reddit_data(json_input_path, csv_output_path):\n",
    "    with open(json_input_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Lists to store the processed data\n",
    "    processed_data = []\n",
    "\n",
    "    # Process each submission and its comments\n",
    "    for submission in data:\n",
    "        submission_text = submission['title']\n",
    "        if submission['text']:  # Combine text content if it exists\n",
    "            submission_text += \" \" + submission['text']\n",
    "            \n",
    "        # Add submission data\n",
    "        processed_data.append({\n",
    "            'subreddit': submission['subreddit'],\n",
    "            'id': submission['post_id'],\n",
    "            'type': f\"submission_{submission['sort_type']}\",  # submission_hot or submission_con\n",
    "            'depth': 0,  # submissions are depth 0\n",
    "            'score': submission['score'],\n",
    "            'time': submission['created_utc'],\n",
    "            'text': submission_text\n",
    "        })\n",
    "        \n",
    "        # Process comments\n",
    "        for comment in submission['comments']:\n",
    "            processed_data.append({\n",
    "                'subreddit': submission['subreddit'],\n",
    "                'id': comment['comment_id'],\n",
    "                'type': 'comment',\n",
    "                'depth': comment['depth'] + 1,  # increment depth by 1\n",
    "                'score': comment['score'],\n",
    "                'time': comment['created_utc'],\n",
    "                'text': comment['body']\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    # Print some basic statistics\n",
    "    print(\"\\nFile convert complete. Results saved to data/processed-data/text_inital.json.\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(\"\\nDistribution by type:\")\n",
    "    print(df['type'].value_counts())\n",
    "    print(\"\\nDepth distribution:\")\n",
    "    print(df['depth'].value_counts().sort_index())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Execute the conversion\n",
    "file_path_text_raw = \"data/raw-data/text_raw.json\"\n",
    "file_path_text_initial = \"data/processed-data/text_initial.csv\"  \n",
    "\n",
    "df_text_initial = convert_reddit_data(file_path_text_raw, file_path_text_initial)\n",
    "df_text_initial.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "\n",
    "# Download NLTK stopwords and WordNet data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove newline characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace characters (including newline, tabs) with a single space\n",
    "    \n",
    "    # Remove meaningless characters, such as special symbols \n",
    "    text = re.sub(r'[^\\w\\s,.\\-]', '', text)  # Keep letters, numbers, spaces, commas, periods, and hyphens\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'@\\S+', '', text)  # Remove @username mentions\n",
    "    text = re.sub(r'#[\\w]+', '', text)  # Remove all hashtags (e.g., #aiart)\n",
    "    text = re.sub(r'-', '', text)  # Remove hyphens\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')  \n",
    "    \n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove double quotes\n",
    "    text = text.replace('\"', '')  \n",
    "\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        text = text[1:-1]\n",
    "\n",
    "    return text\n",
    "\n",
    "def remove_non_english_words(text):\n",
    "    # English words only\n",
    "    english_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "    return ' '.join(english_words)\n",
    "\n",
    "def process_text(text):\n",
    "    # Tokenize the text into words\n",
    "    words = text.split()\n",
    "    # Remove stop words and apply lemmatization\n",
    "    processed_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    # Join the processed words back into a string\n",
    "    return ' '.join(processed_words)\n",
    "\n",
    "# Clean the text of each row in the 'text' column\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: clean_text(x))  # Clean the 'text' field\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: remove_non_english_words(x))  # Keep only English words\n",
    "df_text_initial['text'] = df_text_initial['text'].apply(lambda x: process_text(x))  # Remove stop words and apply lemmatization\n",
    "\n",
    "# Drop duplicate rows based on the 'text' column\n",
    "df_text_clean = df_text_initial.drop_duplicates(subset=['text'])\n",
    "\n",
    "# Output the cleaned DataFrame to a CSV file\n",
    "file_path_text_clean = \"data/processed-data/text_clean.csv\"\n",
    "df_text_clean.to_csv(file_path_text_clean, index=False, encoding='utf-8')\n",
    "print(f\"Data cleaning complete. Results saved to {file_path_text_clean}\")\n",
    "df_text_clean.head(6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
