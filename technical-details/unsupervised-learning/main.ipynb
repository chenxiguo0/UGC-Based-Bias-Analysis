{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Unsupervised Learning\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- After digesting the instructions, you can delete this cell, these are assignment instructions and do not need to be included in your final submission.  -->\n",
    "\n",
    "{{< include unsupervised.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path_text_clean = \"data/processed-data/text_clean.csv\"\n",
    "df = pd.read_csv(file_path_text_clean)\n",
    "\n",
    "# Additional domain-specific stopwords\n",
    "domain_stopwords = {\n",
    "    'trump', 'biden', 'republican', 'democrat', 'like', 'im', 'dont', 'people',\n",
    "    'think', 'know', 'would', 'said', 'one', 'year', 'state', 'time'\n",
    "}\n",
    "\n",
    "# Load SpaCy model for better text processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def improved_text_preprocessing(text):\n",
    "    # Handle non-string inputs\n",
    "    if not isinstance(text, str):\n",
    "        if pd.isna(text):  \n",
    "            return \"\"\n",
    "        text = str(text)  \n",
    "    \n",
    "    doc = nlp(text)\n",
    "    # Keep only nouns, adjectives, verbs, and adverbs\n",
    "    tokens = [token.lemma_.lower() for token in doc \n",
    "             if (token.pos_ in ['NOUN', 'ADJ', 'VERB', 'ADV']) \n",
    "             and (token.lemma_.lower() not in domain_stopwords)\n",
    "             and (len(token.lemma_) > 2)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Convert text column to string type and handle NaN values\n",
    "df['text'] = df['text'].fillna(\"\").astype(str)\n",
    "\n",
    "df['text'] = df['text'].apply(improved_text_preprocessing)\n",
    "submissions = df[df['type'].str.contains('submission')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dimensionality_reduction(tfidf_matrix):\n",
    "    \"\"\"\n",
    "    Perform and compare PCA and t-SNE dimensionality reduction\n",
    "    \"\"\"\n",
    "    # Convert sparse matrix to dense\n",
    "    X_dense = tfidf_matrix.toarray()\n",
    "    \n",
    "    # 1. PCA Analysis\n",
    "    print(\"\\nPerforming PCA analysis...\")\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_dense)\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    cumsum_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Plot explained variance ratio\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(cumsum_variance_ratio) + 1), cumsum_variance_ratio)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('PCA: Explained Variance Ratio vs Number of Components')\n",
    "    plt.axhline(y=0.8, color='r', linestyle='--', label='80% Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal number of components for 80% variance\n",
    "    n_components_80 = np.argmax(cumsum_variance_ratio >= 0.8) + 1\n",
    "    print(f\"Number of components needed for 80% variance: {n_components_80}\")\n",
    "    \n",
    "    # 2. t-SNE Analysis with different perplexity values\n",
    "    perplexities = [5, 30, 50]\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # First reduce dimensionality with PCA to 50 components for efficiency\n",
    "    pca_50 = PCA(n_components=50)\n",
    "    X_pca_50 = pca_50.fit_transform(X_dense)\n",
    "    \n",
    "    for idx, perp in enumerate(perplexities):\n",
    "        print(f\"\\nPerforming t-SNE with perplexity {perp}...\")\n",
    "        tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X_pca_50)\n",
    "        \n",
    "        plt.subplot(1, 3, idx + 1)\n",
    "        plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5)\n",
    "        plt.title(f't-SNE (perplexity={perp})')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get optimal PCA and scale to non-negative values\n",
    "    pca_optimal = PCA(n_components=n_components_80)\n",
    "    X_pca_optimal = pca_optimal.fit_transform(X_dense)\n",
    "    \n",
    "    # Scale PCA results to [0, 1] range for NMF\n",
    "    scaler = MinMaxScaler()\n",
    "    X_pca_scaled = scaler.fit_transform(X_pca_optimal)\n",
    "    \n",
    "    # Final t-SNE\n",
    "    tsne_final = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "    X_tsne_final = tsne_final.fit_transform(X_pca_50)\n",
    "    \n",
    "    return X_pca_scaled, X_tsne_final, pca_optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_nmf_analysis(texts, n_topics=5):\n",
    "    # Create TF-IDF vectors\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2)\n",
    "    )\n",
    "    tfidf = tfidf_vectorizer.fit_transform(texts)\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    X_pca, X_tsne, pca_model = perform_dimensionality_reduction(tfidf)\n",
    "    \n",
    "    # Apply NMF on scaled PCA-reduced data\n",
    "    nmf_model = NMF(\n",
    "        n_components=n_topics,\n",
    "        random_state=42\n",
    "    )\n",
    "    nmf_topics = nmf_model.fit_transform(X_pca)\n",
    "    \n",
    "    # Visualize topics in reduced dimensional space\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot PCA\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=np.argmax(nmf_topics, axis=1), cmap='tab10')\n",
    "    plt.title('Topics visualized in PCA space')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    \n",
    "    # Plot t-SNE\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=np.argmax(nmf_topics, axis=1), cmap='tab10')\n",
    "    plt.title('Topics visualized in t-SNE space')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return np.argmax(nmf_topics, axis=1), nmf_model, tfidf_vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_dbscan_validation(texts, tfidf_vectorizer):\n",
    "    print(\"\\nPerforming DBSCAN validation...\")\n",
    "    \n",
    "    # Create TF-IDF vectors\n",
    "    tfidf = tfidf_vectorizer.transform(texts)\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    X_pca, X_tsne, _ = perform_dimensionality_reduction(tfidf)\n",
    "    \n",
    "    # Apply DBSCAN on t-SNE results\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_labels = dbscan.fit_predict(X_tsne)\n",
    "    \n",
    "    # Visualize clustering results\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=dbscan_labels, cmap='tab10')\n",
    "    plt.title('DBSCAN Clustering Results (t-SNE space)')\n",
    "    plt.xlabel('t-SNE 1')\n",
    "    plt.ylabel('t-SNE 2')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.show()\n",
    "    \n",
    "    # Count number of clusters\n",
    "    n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "    print(f\"DBSCAN found {n_clusters} clusters\")\n",
    "    print(f\"Number of samples in each cluster: {pd.Series(dbscan_labels).value_counts().sort_index()}\")\n",
    "    \n",
    "    return dbscan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all methods\n",
    "print(\"Performing topic analysis...\")\n",
    "\n",
    "# NMF Analysis with dimensionality reduction\n",
    "nmf_topics, nmf_model, tfidf_vectorizer = perform_nmf_analysis(submissions['text'])\n",
    "submissions['nmf_topic'] = nmf_topics\n",
    "\n",
    "# Function to display top terms for each topic\n",
    "def display_top_terms(model, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_terms = [feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_terms)}\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nNMF Topics:\")\n",
    "display_top_terms(nmf_model, tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# DBSCAN Clustering\n",
    "submissions['dbscan_cluster'] = perform_dbscan_validation(submissions['text'], tfidf_vectorizer)\n",
    "\n",
    "# Compare NMF and DBSCAN results\n",
    "print(\"\\nComparison of NMF topics and DBSCAN clusters:\")\n",
    "print(pd.crosstab(submissions['nmf_topic'], submissions['dbscan_cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simplified topic labels (select the top term)\n",
    "topic_labels = {\n",
    "    0: \"abortion\",  \n",
    "    1: \"guns\",      \n",
    "    2: \"tax\",       \n",
    "    3: \"climate\",   \n",
    "    4: \"politics\"   \n",
    "}\n",
    "\n",
    "# Add topic labels to submissions\n",
    "submissions['nmf_topic'] = submissions['nmf_topic'].map(topic_labels)\n",
    "\n",
    "\n",
    "# Create a new column 'nmf_topic' to store the topic label for each row\n",
    "df['nmf_topic'] = None\n",
    "\n",
    "# Variables to store the current submission ID and its topic\n",
    "current_submission_id = None\n",
    "current_topic = None\n",
    "\n",
    "# Iterate through the dataset and assign the topic of the parent submission to each comment\n",
    "for idx, row in df.iterrows():\n",
    "    if 'submission' in row['type']:  # If the current row is a submission\n",
    "        # Get the current submission's ID and its topic\n",
    "        current_submission_id = row['id']\n",
    "        current_topic = submissions[submissions['id'] == current_submission_id]['nmf_topic'].values\n",
    "        if current_topic.size > 0:\n",
    "            current_topic = current_topic[0]\n",
    "        df.at[idx, 'nmf_topic'] = current_topic  \n",
    "    # If the row is a comment, assign the parent submission's topic\n",
    "    if 'comment' in row['type'] and current_submission_id is not None:\n",
    "        df.at[idx, 'nmf_topic'] = current_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df_text_topic = df\n",
    "\n",
    "file_path_text_topic = \"data/processed-data/text_topic.csv\"\n",
    "df_text_topic.to_csv(file_path_text_topic, index=False)\n",
    "\n",
    "print(f\"\\nModeling complete. Results saved to {file_path_text_topic}\")\n",
    "df_text_topic.head(6)\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nSample results:\")\n",
    "df_text_topic.head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
